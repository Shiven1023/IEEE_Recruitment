{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "58a14f71-6361-4f14-8c45-feb9ff42e79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, numpy as np, matplotlib.pyplot as plt\n",
    "folder = \"/Users/raaghav/Downloads/final\" # folder containing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5c51c67d-cbce-4d20-aaba-c5cca5660d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 9 class\n",
      "Loaded 0 class\n",
      "Loaded 7 class\n",
      "Loaded 6 class\n",
      "Loaded 1 class\n",
      "Loaded 8 class\n",
      "Loaded 4 class\n",
      "Loaded 3 class\n",
      "Loaded 2 class\n",
      "Loaded 5 class\n",
      "After reshaping\n",
      "(60000, 784) (60000, 10)\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.00392157 0.00784314 0.\n",
      " 0.03921569 0.5137255  0.39607844 0.44313726 0.49411765 0.4\n",
      " 0.23529412 0.2509804  0.33333334 0.3372549  0.36862746 0.35686275\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.00392157\n",
      " 0.         0.         0.         0.         0.21176471 0.53333336\n",
      " 0.6313726  0.7607843  0.7882353  0.85882354 0.84705883 0.8352941\n",
      " 0.85490197 0.78431374 0.77254903 0.68235296 0.02352941 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.5372549  0.52156866 0.20392157 0.29411766 0.5372549\n",
      " 0.58431375 0.49019608 0.59607846 0.5137255  0.3372549  0.7137255\n",
      " 0.6901961  0.7019608  0.13725491 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.01176471 0.         0.10588235 0.80784315\n",
      " 0.7137255  0.7019608  0.56078434 0.5686275  0.60784316 0.40392157\n",
      " 0.44313726 0.5254902  0.48235294 0.40392157 0.36078432 0.56078434\n",
      " 0.34901962 0.         0.         0.         0.         0.\n",
      " 0.00392157 0.         0.         0.         0.00392157 0.01568628\n",
      " 0.         0.00784314 0.49411765 0.19607843 0.4745098  0.77254903\n",
      " 0.7176471  0.64705884 0.70980394 0.827451   0.84705883 0.65882355\n",
      " 0.52156866 0.7490196  0.68235296 0.6627451  0.5372549  0.\n",
      " 0.         0.         0.         0.00392157 0.00392157 0.\n",
      " 0.         0.         0.00784314 0.01960784 0.         0.30980393\n",
      " 0.6901961  0.49411765 0.49019608 0.53333336 0.7372549  0.70980394\n",
      " 0.7254902  0.7019608  0.87058824 0.5137255  0.6784314  1.\n",
      " 0.84313726 0.7137255  0.8392157  0.20392157 0.         0.\n",
      " 0.01176471 0.01568628 0.01176471 0.01568628 0.01568628 0.00784314\n",
      " 0.         0.         0.18039216 0.48235294 0.40392157 0.48235294\n",
      " 0.54509807 0.6        0.61960787 0.7882353  0.8156863  0.73333335\n",
      " 0.6745098  0.4117647  0.5137255  0.72156864 0.7254902  0.7254902\n",
      " 0.8745098  0.18431373 0.00784314 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.19607843\n",
      " 0.46666667 0.39607844 0.35686275 0.3764706  0.41960785 0.5372549\n",
      " 0.5568628  0.6        0.7294118  0.8039216  0.84313726 0.70980394\n",
      " 0.7019608  0.7647059  0.7647059  0.7019608  0.7647059  0.00392157\n",
      " 0.         0.         0.08235294 0.13333334 0.15294118 0.10196079\n",
      " 0.05882353 0.16470589 0.33333334 0.42745098 0.3647059  0.4745098\n",
      " 0.4        0.43529412 0.47058824 0.4745098  0.50980395 0.5294118\n",
      " 0.5411765  0.627451   0.6901961  0.79607844 0.6745098  0.54901963\n",
      " 0.6627451  0.59607846 0.7921569  0.24313726 0.         0.44313726\n",
      " 0.45882353 0.5137255  0.5137255  0.47058824 0.43137255 0.4627451\n",
      " 0.4509804  0.41960785 0.43137255 0.4745098  0.56078434 0.5137255\n",
      " 0.5647059  0.52156866 0.5568628  0.5137255  0.49019608 0.5294118\n",
      " 0.6392157  0.6784314  0.59607846 0.6156863  0.54509807 0.5294118\n",
      " 0.6509804  0.08627451 0.31764707 0.6509804  0.5137255  0.4862745\n",
      " 0.4509804  0.39607844 0.40784314 0.45490196 0.4745098  0.4627451\n",
      " 0.5137255  0.49019608 0.5254902  0.5882353  0.5019608  0.48235294\n",
      " 0.5764706  0.6        0.6666667  0.65882355 0.75686276 0.67058825\n",
      " 0.68235296 0.6627451  0.6        0.6        0.6156863  0.01176471\n",
      " 0.07058824 0.45882353 0.63529414 0.6627451  0.6313726  0.5647059\n",
      " 0.49411765 0.45882353 0.4509804  0.4509804  0.4745098  0.5137255\n",
      " 0.56078434 0.5921569  0.6627451  0.6509804  0.6313726  0.7647059\n",
      " 0.7607843  0.65882355 0.63529414 0.5411765  0.5254902  0.5019608\n",
      " 0.52156866 0.5137255  0.6117647  0.05098039 0.         0.\n",
      " 0.14117648 0.4117647  0.58431375 0.6745098  0.7176471  0.67058825\n",
      " 0.6313726  0.58431375 0.627451   0.6745098  0.7411765  0.6745098\n",
      " 0.6392157  0.53333336 0.39607844 0.32941177 0.05882353 0.39215687\n",
      " 0.7921569  0.6392157  0.60784316 0.627451   0.6745098  0.69803923\n",
      " 0.81960785 0.16470589 0.         0.         0.         0.\n",
      " 0.         0.06666667 0.16862746 0.26666668 0.3647059  0.36862746\n",
      " 0.28235295 0.20392157 0.12156863 0.07450981 0.         0.\n",
      " 0.         0.         0.         0.22745098 0.58431375 0.38039216\n",
      " 0.36078432 0.3372549  0.28627452 0.25490198 0.3019608  0.01568628\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ] [0 0 0 0 0 0 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "# Now we try to load the images and their corresponding labels into memory\n",
    "\n",
    "def load_data(X, y):\n",
    "    for f in os.listdir(folder):\n",
    "        class_path = os.path.join(folder,f)\n",
    "        if not os.path.isdir(class_path):\n",
    "            continue \n",
    "        for file in os.listdir(f\"{folder}/{f}\"):\n",
    "            img = plt.imread(f\"{folder}/{f}/{file}\")\n",
    "            X.append(img)\n",
    "\n",
    "            # The most obvious choice for the label is the class (folder) name\n",
    "            # label = int(f)\n",
    "\n",
    "            # [Q1] But we dont use it here why? Why is it an array of 10 elements?\n",
    "            # Clue: Lookup one hot encoding\n",
    "            # Read up on Cross Entropy Loss\n",
    "            #--> Since one hot encoding requires a one-hot vector(i.e 10 elements for 10 categories)\n",
    "            #--> Cross entropy loss expects probabilities (not class indices).\n",
    "            label = [0] * 10\n",
    "            label[int(f)] = 1 # Why is this array the label and not a numeber?\n",
    "\n",
    "            y.append(label)\n",
    "            \n",
    "        print(f\"Loaded {f} class\")\n",
    "\n",
    "X, y = [], []\n",
    "load_data(X, y)\n",
    "\n",
    "# [Q2] Why convert to numpy array?\n",
    "\"\"\"--> more memory efficient than python lists\n",
    "--> allows for vectorisation\"\"\"\n",
    "\"\"\"\n",
    ".\n",
    "convert x and y to numpy arrays here\n",
    ".\n",
    "\"\"\"\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "X = X[:, :,:, 0] \n",
    "# [Q3] Why are we doing this and what does this type of slicing result in?\n",
    "# --> this results in us getting only the zeroeth index or first channel,i.e it discards colour and keeps grayscale intensity \n",
    "X = X.reshape(X.shape[0], X.shape[1]*X.shape[2]) # [Q4] Why are we reshaping the data?\n",
    "print(\"After reshaping\")\n",
    "print(X.shape, y.shape)\n",
    "print(X[0], y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "863e794b-395c-4d44-b986-06858ac7344b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN:\n",
    "    def __init__(self, input_neurons, hidden_neurons, output_neurons, learning_rate, epochs):\n",
    "        \"\"\"\n",
    "        Class Definition\n",
    "        \n",
    "        We use a class because it is easy to visualize the process of training a neural network\n",
    "        It's also easier to resuse and repurpose depending on the task at hand\n",
    "\n",
    "        We have a simple neural network, with an input layer, one hidden (middle) layer and an output layer\n",
    "\n",
    "        input_neurons: Number of neurons in the input layer\n",
    "        hidden_neurons: Number of neurons in the hidden layer\n",
    "        output_neurons: Number of neurons in the output layer\n",
    "        learning_rate: The rate at which the weights are updated [Q5] What is the learning rate?\n",
    "        epochs: Number of times the model will train on the entire dataset \n",
    "        \"\"\"\n",
    "\n",
    "        self.input_neurons = input_neurons\n",
    "        self.hidden_neurons = hidden_neurons\n",
    "        self.output_neurons = output_neurons\n",
    "        self.epochs = epochs\n",
    "\n",
    "        self.lr = learning_rate\n",
    "\n",
    "        \"\"\"\n",
    "        Weights and Biases\n",
    "\n",
    "        At this point you should know what weights and biases are in a neural network and if not, go check out the 3blue1brown video on Neural Networks\n",
    "        What matters here is however the matrix dimensions of the weights and biases\n",
    "\n",
    "        [Q6] Why are the dimensions of the weights and biases the way they are?\n",
    "        \n",
    "        Try to figure out the dimensions of the weights and biases for the hidden and output layers\n",
    "        Try to see what equations represent the forward pass (basically the prediction)\n",
    "        And then, try to see if the dimensions of the matrix multiplications are correct\n",
    "\n",
    "        Note: The bias dimensions may not match. Look up broadcasting in numpy to understand\n",
    "        [Q7] What is broadcasting and why do we need to broadcast the bias?\n",
    "        \"\"\"\n",
    "\n",
    "        # Ideally any random set of weights and biases can be used to initialize the network\n",
    "        # self.wih = np.random.randn(hidden_neurons, input_neurons)\n",
    "\n",
    "        # [Q8] What is np.random.randn? What's the shape of this matrix?\n",
    "\n",
    "        # Optional: Try to figure out why the weights are initialized this way\n",
    "        # Note: You can just use the commented out line above if you don't want to do this\n",
    "\n",
    "        self.wih = np.random.randn(hidden_neurons, input_neurons) * np.sqrt(2/input_neurons)\n",
    "        self.bih = np.zeros((hidden_neurons, 1))\n",
    "\n",
    "        self.who = np.random.randn(output_neurons, hidden_neurons) * np.sqrt(2/hidden_neurons)\n",
    "        self.bho = np.zeros((output_neurons, 1))\n",
    "\n",
    "    # Activation Functions and their derivatives\n",
    "    # [Q9] What are activation functions and why do we need them?\n",
    "\n",
    "    def relu(self, z):\n",
    "        \"\"\"\n",
    "        Implementation of the RELU function\n",
    "        z: (n, 1)\n",
    "        returns (n, 1)\n",
    "        \"\"\"\n",
    "        return z * (z > 0)\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        \"\"\"\n",
    "        Implementation of the Sigmoid function\n",
    "        z: (n, 1)\n",
    "        returns (n, 1)\n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def relu_derivative(self, z):\n",
    "        \"\"\"\n",
    "        Implementation of the RELU derivative function\n",
    "        z: (n, 1)\n",
    "        returns (n, 1)\n",
    "        \"\"\"\n",
    "        return 1 * (z > 0)\n",
    "\n",
    "    def sigmoid_derivative(self, z):\n",
    "        \"\"\"\n",
    "        Implementation of the Sigmoid derivative function\n",
    "        z: (n, 1)\n",
    "        returns (n, 1)\n",
    "        \"\"\"\n",
    "        return z * (1 - z)\n",
    "\n",
    "    # [Q10] What is the softmax function and why do we need it? Read up on it\n",
    "    def softmax(self, z):\n",
    "        \"\"\"\n",
    "        Implementation of the Softmax function\n",
    "        z: (n, 1)\n",
    "        returns (n, 1)\n",
    "        \"\"\"\n",
    "        return np.exp(z) / np.sum(np.exp(z), axis=0)\n",
    "\n",
    "    def softmax_derivative(self, z):\n",
    "        \"\"\"\n",
    "        Implementation of the Softmax derivative function\n",
    "        z: (n, 1)\n",
    "        returns (n, 1)\n",
    "        \"\"\"\n",
    "        return z * (1 - z)\n",
    "\n",
    "    # Loss Functions and their derivatives\n",
    "    # [Q11] What are loss functions and why do we need them?\n",
    "\n",
    "    def mean_squared_error(self, y, y_hat):\n",
    "        \"\"\"\n",
    "        Implementation of the Mean Squared Error function\n",
    "        y: (10, n)\n",
    "        y_hat: (10, n)\n",
    "        returns (1, n)\n",
    "        \"\"\"\n",
    "        return np.mean((y - y_hat) ** 2, axis=0)\n",
    "\n",
    "    def cross_entropy_loss(self, y, y_hat):\n",
    "        \"\"\"\n",
    "        Implementation of the Cross Entropy Loss function\n",
    "        y: (10, n)\n",
    "        y_hat: (10, n)\n",
    "        returns (1, n)\n",
    "        \"\"\"\n",
    "\n",
    "        # Implement the cross entropy loss function here and return it\n",
    "        # Keep the dimensions of the input in mind when writing the code\n",
    "\n",
    "        # [Code Goes Here]\n",
    "\n",
    "        eps = 1e-12\n",
    "        y_hat = np.clip(y_hat, eps, 1 - eps)  # avoid log(0)\n",
    "        return -np.sum(y * np.log(y_hat)) / y.shape[1]\n",
    "\n",
    "    def mean_squared_error_derivative(self, y, y_hat):\n",
    "        \"\"\"\n",
    "        Implementation of the Mean Squared Error derivative function\n",
    "        y: (10, n)\n",
    "        y_hat: (10, n)\n",
    "        returns (10, n)\n",
    "        \"\"\"\n",
    "        return y_hat - y\n",
    "\n",
    "    def cross_entropy_derivative(self, y, y_hat):\n",
    "        \"\"\"\n",
    "        Implementation of the Cross Entropy Loss derivative function\n",
    "        y: (10, n)\n",
    "        y_hat: (10, n)\n",
    "        returns (10, n)\n",
    "        \"\"\"\n",
    "\n",
    "        # Implement the cross entropy loss derivative function here and return it\n",
    "        # Note: The derivative of the CEL is usually taken with respect to the softmax input not output so keep that in mind while writing\n",
    "\n",
    "        # [Code Goes Here]\n",
    "\n",
    "        return (y_hat - y)\n",
    "\n",
    "    # Forward propagation\n",
    "    def forward(self, input_list):\n",
    "        \"\"\"\n",
    "        Implementation of the Forward Pass\n",
    "        input_list: (784, n)        - n is the number of images\n",
    "        returns (10, n)              - n is the number of images\n",
    "    \n",
    "        Now we come to the heart of the neural network, the forward pass\n",
    "        This is where the input is passed through the network to get the output\n",
    "\n",
    "        [Q12] What does the output choice we have here mean? It's an array of 10 elements per image, but why?\n",
    "        \"\"\"\n",
    "\n",
    "        inputs = np.array(input_list, ndmin=2).T\n",
    "        inputs = inputs - np.mean(inputs) # [Q13] Why are we subtracting the mean of the inputs?\n",
    "\n",
    "\n",
    "        \n",
    "        # To get to the hidden layer:\n",
    "        # Multiply the input with the weights and adding the bias\n",
    "        # Apply the activation function (relu in this case)\n",
    "\n",
    "        # [Code Goes Here]\n",
    "        hidden_inputs = np.dot(self.wih, inputs) + self.bih\n",
    "        hidden_outputs = self.relu(hidden_inputs)\n",
    "        \n",
    "\n",
    "        # To get to the output layer:\n",
    "        # Multiply the hidden layer output with the weights and adding the bias\n",
    "        # Apply the activation function (softmax in this case)\n",
    "        # [Q14] Why are we using the softmax function here?\n",
    "\n",
    "        # [Code Goes Here]\n",
    "        final_inputs = np.dot(self.who, hidden_outputs) + self.bho\n",
    "        final_outputs = self.softmax(final_inputs)\n",
    "        \n",
    "\n",
    "        # Return it\n",
    "\n",
    "        # [Code Goes Here]\n",
    "        return final_outputs\n",
    "        \n",
    "\n",
    "    # Back propagation\n",
    "    def backprop(self, inputs_list, targets_list):\n",
    "        \"\"\"\n",
    "        Implementation of the Backward Pass\n",
    "        inputs_list: (784, n)\n",
    "        targets_list: (10, n)\n",
    "        returns a scalar value (loss)\n",
    "        \n",
    "        This is where the magic happens, the backpropagation algorithm\n",
    "        This is where the weights are updated based on the error in the prediction of the network\n",
    "\n",
    "        Now, the calculus involved is fairly complicated, especially because it's being done in matrix form\n",
    "        However the intuition is simple. \n",
    "\n",
    "        Since this is a recruitment stage, most of the function is written out for you, so follow along with the comments\n",
    "        \"\"\"\n",
    "\n",
    "        # Basic forward pass to get the outputs\n",
    "        # Obviously we need the predictions to know how the model is doing\n",
    "        # [Q15] Why are we doing a forward pass here instead of just using the outputs from the forward function?\n",
    "        # Is there any actual reason, or could we just swap it?\n",
    "\n",
    "        inputs = np.array(inputs_list, ndmin=2).T # (784, n)\n",
    "        inputs = inputs - np.mean(inputs)\n",
    "        \n",
    "\n",
    "\n",
    "        tj = np.array(targets_list, ndmin=2).T # (10, n)\n",
    "\n",
    "        hidden_inputs = np.dot(self.wih, inputs) + self.bih\n",
    "        hidden_outputs = self.relu(hidden_inputs)\n",
    "\n",
    "        final_inputs = np.dot(self.who, hidden_outputs) + self.bho\n",
    "        yj = self.softmax(final_inputs)\n",
    "\n",
    "        # Calculating the loss - This is the error in the prediction\n",
    "        # The loss then is the indication of how well the model is doing, its a useful parameter to track to see if the model is improving\n",
    "\n",
    "        loss = self.cross_entropy_loss(tj, yj)\n",
    "\n",
    "\n",
    "        # Updating the weights using Update Rule\n",
    "        # Now that we have the incorrect predictions, we can update the weights to make the predictions better\n",
    "        # This is done using the gradient of the loss function with respect to the weights\n",
    "        # Basically, we know how much the overall error is caused due to individual weights using the chain rule of calculus\n",
    "        # Since we want to minimise the error, we move in the opposite direction of something like a \"derivative\" of the error with respect to the weights\n",
    "        # Calculus therefore helps us find the direction in which we should move to reduce the error\n",
    "        # A direction means what delta W changes we need to make to make the model better\n",
    "\n",
    "        # Output Layer - We start with the output layer because we are backtracking how the error is caused\n",
    "        # Think of it as using the derivatives of each layer while going back\n",
    "\n",
    "\n",
    "        # For the task, you will be using Cross Entropy Loss\n",
    "\n",
    "        # Change this to cross entropy loss\n",
    "        dE_dzo = self.cross_entropy_derivative(tj,yj) # (10,n)\n",
    "        # Note: the derivative of the CEL is usually taken with respect to the softmax input not output so keep that in mind while writing\n",
    "\n",
    "\n",
    "        dE_dwho = np.dot(dE_dzo, hidden_outputs.T) / hidden_outputs.shape[1] # dot((10,n) (n,128) = (10,128)\n",
    "        dE_dbho = np.mean(dE_dzo, axis=1, keepdims=True) # sum((10,n), axis=1) = (10,1)\n",
    "        \n",
    "        self.who -= self.lr * dE_dwho\n",
    "        self.bho -= self.lr * dE_dbho\n",
    "\n",
    "        # Hidden Layer\n",
    "        dE_dah = np.dot(self.who.T, dE_dzo) # dot((128,10), (10,n)) = (128,n)\n",
    "        dE_dzh = dE_dah * self.relu_derivative(hidden_inputs)\n",
    "        dE_dwih = np.dot(dE_dzh, inputs.T) / inputs.shape[1]\n",
    "        dE_dbih = np.mean(dE_dzh, axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "        self.wih -= self.lr * dE_dwih\n",
    "        self.bih -= self.lr * dE_dbih\n",
    "\n",
    "        return np.mean(loss)\n",
    "\n",
    "    def fit(self, inputs_list, targets_list,validation_data, validation_labels):\n",
    "        \"\"\"\n",
    "        Implementation of the training loop\n",
    "        inputs_list: (784, n)\n",
    "        targets_list: (10, n)\n",
    "        validation_data: (784, n)\n",
    "        validation_labels: (10, n)\n",
    "        returns train_loss, val_loss\n",
    "\n",
    "        This is where the training loop is implemented\n",
    "        We loop over the entire dataset for a certain number of epochs\n",
    "        We also track the validation loss to see how well the model is generalizing\n",
    "        [Q16] What is the validation dataset and what do we mean by generalization?\n",
    "\n",
    "        We also return the training and validation loss to see how the model is improving\n",
    "        It's a good idea to plot these to see how the model is doing\n",
    "        \"\"\"\n",
    "\n",
    "        train_loss = []\n",
    "        val_loss = []\n",
    "        for epoch in range(self.epochs):\n",
    "            loss = self.backprop(inputs_list, targets_list)\n",
    "            train_loss.append(loss)\n",
    "            vloss = self.cross_entropy_loss(validation_labels.T, self.forward(validation_data))\n",
    "            val_loss.append(np.mean(vloss)) \n",
    "            print(f\"Epoch: {epoch}, Loss: {loss}, Val Loss: {val_loss[-1]}\")\n",
    "\n",
    "        return train_loss[1:], val_loss[:-1] \n",
    "\n",
    "    def predict(self, X):\n",
    "        outputs = self.forward(X).T\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "35ed77e5-a522-4e7d-bd95-e4f23a21ca58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 2.345052103829998, Val Loss: 2.207802519656411\n",
      "Epoch: 1, Loss: 2.206148331032214, Val Loss: 2.0881321512210205\n",
      "Epoch: 2, Loss: 2.0863595776421366, Val Loss: 1.9830851297222218\n",
      "Epoch: 3, Loss: 1.9811754485642266, Val Loss: 1.889810829091333\n",
      "Epoch: 4, Loss: 1.8877911334383686, Val Loss: 1.8064396162684042\n",
      "Epoch: 5, Loss: 1.8043620151869475, Val Loss: 1.7316028332604143\n",
      "Epoch: 6, Loss: 1.7294923161963207, Val Loss: 1.664242715531025\n",
      "Epoch: 7, Loss: 1.662127526951412, Val Loss: 1.6034544993349633\n",
      "Epoch: 8, Loss: 1.601363780820653, Val Loss: 1.5484434301858565\n",
      "Epoch: 9, Loss: 1.5464138194821375, Val Loss: 1.4985253037338975\n",
      "Epoch: 10, Loss: 1.496561284548252, Val Loss: 1.453068766896672\n",
      "Epoch: 11, Loss: 1.4511935841553547, Val Loss: 1.4115475655497958\n",
      "Epoch: 12, Loss: 1.4097810642471944, Val Loss: 1.3735117878757992\n",
      "Epoch: 13, Loss: 1.3718693047230397, Val Loss: 1.338560612847011\n",
      "Epoch: 14, Loss: 1.337041412715674, Val Loss: 1.3063545259217053\n",
      "Epoch: 15, Loss: 1.304955026693699, Val Loss: 1.276588378308507\n",
      "Epoch: 16, Loss: 1.275308449211607, Val Loss: 1.249010533673679\n",
      "Epoch: 17, Loss: 1.2478582439255124, Val Loss: 1.2234120760283662\n",
      "Epoch: 18, Loss: 1.2223885360769093, Val Loss: 1.1996079893412779\n",
      "Epoch: 19, Loss: 1.1987064829489829, Val Loss: 1.1774141025375922\n",
      "Epoch: 20, Loss: 1.1766288128272502, Val Loss: 1.1566735576094425\n",
      "Epoch: 21, Loss: 1.1560027519652865, Val Loss: 1.1372564172429411\n",
      "Epoch: 22, Loss: 1.13669492578244, Val Loss: 1.119052062767318\n",
      "Epoch: 23, Loss: 1.1185916617342726, Val Loss: 1.1019476769268313\n",
      "Epoch: 24, Loss: 1.101586030685999, Val Loss: 1.0858504037649142\n",
      "Epoch: 25, Loss: 1.0855827404946436, Val Loss: 1.0706780525211894\n",
      "Epoch: 26, Loss: 1.070502335670651, Val Loss: 1.0563543522313552\n",
      "Epoch: 27, Loss: 1.056267053758178, Val Loss: 1.0428104364610287\n",
      "Epoch: 28, Loss: 1.0428077246288985, Val Loss: 1.0299875723223801\n",
      "Epoch: 29, Loss: 1.0300650492538552, Val Loss: 1.0178243384029046\n",
      "Epoch: 30, Loss: 1.017977481604657, Val Loss: 1.0062690315600793\n",
      "Epoch: 31, Loss: 1.0064952322669918, Val Loss: 0.9952753441092289\n",
      "Epoch: 32, Loss: 0.9955704193957801, Val Loss: 0.9848003061033411\n",
      "Epoch: 33, Loss: 0.9851620467346459, Val Loss: 0.9748082736247916\n",
      "Epoch: 34, Loss: 0.9752326240672601, Val Loss: 0.9652636317067042\n",
      "Epoch: 35, Loss: 0.9657476983106861, Val Loss: 0.956132768411103\n",
      "Epoch: 36, Loss: 0.9566746899940733, Val Loss: 0.9473863882369316\n",
      "Epoch: 37, Loss: 0.9479857896286839, Val Loss: 0.9390011061188778\n",
      "Epoch: 38, Loss: 0.9396560622573233, Val Loss: 0.9309530772923802\n",
      "Epoch: 39, Loss: 0.931662847142456, Val Loss: 0.9232208862254954\n",
      "Epoch: 40, Loss: 0.9239836406605528, Val Loss: 0.9157840060001197\n",
      "Epoch: 41, Loss: 0.9165974148811892, Val Loss: 0.9086250333429414\n",
      "Epoch: 42, Loss: 0.9094875132472056, Val Loss: 0.9017255570339047\n",
      "Epoch: 43, Loss: 0.9026353440800928, Val Loss: 0.8950717420184272\n",
      "Epoch: 44, Loss: 0.896026984309708, Val Loss: 0.8886510704475356\n",
      "Epoch: 45, Loss: 0.889647139235056, Val Loss: 0.8824503947841458\n",
      "Epoch: 46, Loss: 0.8834843379772258, Val Loss: 0.8764577576784496\n",
      "Epoch: 47, Loss: 0.8775276846815655, Val Loss: 0.8706608617850559\n",
      "Epoch: 48, Loss: 0.8717632744800095, Val Loss: 0.8650498987947588\n",
      "Epoch: 49, Loss: 0.8661816609739477, Val Loss: 0.8596137804658177\n",
      "Epoch: 50, Loss: 0.8607744966332607, Val Loss: 0.8543456966944456\n",
      "Epoch: 51, Loss: 0.8555341817116889, Val Loss: 0.8492366637903523\n",
      "Epoch: 52, Loss: 0.8504516989512576, Val Loss: 0.844277055521589\n",
      "Epoch: 53, Loss: 0.8455181804956747, Val Loss: 0.8394607257863013\n",
      "Epoch: 54, Loss: 0.8407268897407989, Val Loss: 0.8347797519116567\n",
      "Epoch: 55, Loss: 0.8360710238095223, Val Loss: 0.830227894931187\n",
      "Epoch: 56, Loss: 0.8315430146594351, Val Loss: 0.8257996634446161\n",
      "Epoch: 57, Loss: 0.8271361892764385, Val Loss: 0.8214884933664677\n",
      "Epoch: 58, Loss: 0.8228453264157274, Val Loss: 0.8172895545638578\n",
      "Epoch: 59, Loss: 0.8186664530259441, Val Loss: 0.8131993055914012\n",
      "Epoch: 60, Loss: 0.8145948282969964, Val Loss: 0.809212412542796\n",
      "Epoch: 61, Loss: 0.8106247048466986, Val Loss: 0.8053241066979071\n",
      "Epoch: 62, Loss: 0.8067527677517746, Val Loss: 0.8015308658904812\n",
      "Epoch: 63, Loss: 0.8029747386617396, Val Loss: 0.7978276897084353\n",
      "Epoch: 64, Loss: 0.7992864963882409, Val Loss: 0.7942117961231219\n",
      "Epoch: 65, Loss: 0.7956842886884894, Val Loss: 0.7906799476228901\n",
      "Epoch: 66, Loss: 0.7921650351197193, Val Loss: 0.7872292470977003\n",
      "Epoch: 67, Loss: 0.7887258441482282, Val Loss: 0.7838547843359404\n",
      "Epoch: 68, Loss: 0.7853625696648482, Val Loss: 0.7805547470281315\n",
      "Epoch: 69, Loss: 0.7820724519335102, Val Loss: 0.7773238262747595\n",
      "Epoch: 70, Loss: 0.7788519714068922, Val Loss: 0.7741617461477275\n",
      "Epoch: 71, Loss: 0.7756991492407889, Val Loss: 0.7710668130529572\n",
      "Epoch: 72, Loss: 0.7726120266628574, Val Loss: 0.7680376243009905\n",
      "Epoch: 73, Loss: 0.7695899150482735, Val Loss: 0.7650703031438313\n",
      "Epoch: 74, Loss: 0.7666298196539543, Val Loss: 0.7621621418404635\n",
      "Epoch: 75, Loss: 0.7637297633953934, Val Loss: 0.7593112814541086\n",
      "Epoch: 76, Loss: 0.7608866243998396, Val Loss: 0.7565167470208415\n",
      "Epoch: 77, Loss: 0.7580996271626816, Val Loss: 0.753776863439887\n",
      "Epoch: 78, Loss: 0.7553677535834634, Val Loss: 0.7510884812764846\n",
      "Epoch: 79, Loss: 0.7526872266352231, Val Loss: 0.7484518316179665\n",
      "Epoch: 80, Loss: 0.7500572446839722, Val Loss: 0.7458644211144757\n",
      "Epoch: 81, Loss: 0.7474766113322527, Val Loss: 0.7433242704953371\n",
      "Epoch: 82, Loss: 0.7449428060569319, Val Loss: 0.7408302630351229\n",
      "Epoch: 83, Loss: 0.7424550540474367, Val Loss: 0.7383811545488645\n",
      "Epoch: 84, Loss: 0.7400118619556268, Val Loss: 0.7359757076022513\n",
      "Epoch: 85, Loss: 0.7376116406705162, Val Loss: 0.7336109947078685\n",
      "Epoch: 86, Loss: 0.7352524829719526, Val Loss: 0.7312865063260044\n",
      "Epoch: 87, Loss: 0.7329332565466172, Val Loss: 0.7290016504888341\n",
      "Epoch: 88, Loss: 0.7306534599945996, Val Loss: 0.7267557306069405\n",
      "Epoch: 89, Loss: 0.728412452798682, Val Loss: 0.7245475581813617\n",
      "Epoch: 90, Loss: 0.7262087695672846, Val Loss: 0.7223760097806056\n",
      "Epoch: 91, Loss: 0.7240408909762565, Val Loss: 0.7202399029468382\n",
      "Epoch: 92, Loss: 0.7219077444654695, Val Loss: 0.7181384569904519\n",
      "Epoch: 93, Loss: 0.7198089071790028, Val Loss: 0.7160696796505241\n",
      "Epoch: 94, Loss: 0.717742763811533, Val Loss: 0.7140334307264896\n",
      "Epoch: 95, Loss: 0.7157090704469226, Val Loss: 0.7120280077190537\n",
      "Epoch: 96, Loss: 0.7137066851884072, Val Loss: 0.7100540711133556\n",
      "Epoch: 97, Loss: 0.7117353296422051, Val Loss: 0.7081097565577669\n",
      "Epoch: 98, Loss: 0.7097938310463463, Val Loss: 0.7061945029233018\n",
      "Epoch: 99, Loss: 0.7078814601726723, Val Loss: 0.7043078214775149\n",
      "Epoch: 100, Loss: 0.7059975993232871, Val Loss: 0.7024487580759178\n",
      "Epoch: 101, Loss: 0.7041412863992239, Val Loss: 0.7006164170740984\n",
      "Epoch: 102, Loss: 0.7023116868098074, Val Loss: 0.698811184953857\n",
      "Epoch: 103, Loss: 0.7005084381357611, Val Loss: 0.6970317548972231\n",
      "Epoch: 104, Loss: 0.6987306994412519, Val Loss: 0.6952767781855025\n",
      "Epoch: 105, Loss: 0.6969780872794485, Val Loss: 0.6935465652423161\n",
      "Epoch: 106, Loss: 0.6952502652905425, Val Loss: 0.6918403556572921\n",
      "Epoch: 107, Loss: 0.6935466290596002, Val Loss: 0.6901568466523239\n",
      "Epoch: 108, Loss: 0.691866254783328, Val Loss: 0.6884968327805686\n",
      "Epoch: 109, Loss: 0.6902088670892628, Val Loss: 0.6868589115238904\n",
      "Epoch: 110, Loss: 0.6885737572637135, Val Loss: 0.6852427102954941\n",
      "Epoch: 111, Loss: 0.6869605781777995, Val Loss: 0.6836478715796243\n",
      "Epoch: 112, Loss: 0.6853688778869998, Val Loss: 0.6820733058740025\n",
      "Epoch: 113, Loss: 0.6837976245972599, Val Loss: 0.6805189799374549\n",
      "Epoch: 114, Loss: 0.6822461771886145, Val Loss: 0.6789847213057445\n",
      "Epoch: 115, Loss: 0.6807147775618753, Val Loss: 0.6774698789167694\n",
      "Epoch: 116, Loss: 0.6792029422737006, Val Loss: 0.6759735448096903\n",
      "Epoch: 117, Loss: 0.6777096912919197, Val Loss: 0.6744961493287012\n",
      "Epoch: 118, Loss: 0.6762349062794335, Val Loss: 0.6730377879435974\n",
      "Epoch: 119, Loss: 0.6747782795495889, Val Loss: 0.6715966517250533\n",
      "Epoch: 120, Loss: 0.6733387019777174, Val Loss: 0.6701726858034008\n",
      "Epoch: 121, Loss: 0.6719162277477179, Val Loss: 0.6687657078695173\n",
      "Epoch: 122, Loss: 0.670510652179209, Val Loss: 0.6673759132951568\n",
      "Epoch: 123, Loss: 0.6691217693630652, Val Loss: 0.6660029292727516\n",
      "Epoch: 124, Loss: 0.6677491390232883, Val Loss: 0.6646458853146876\n",
      "Epoch: 125, Loss: 0.6663921476154433, Val Loss: 0.6633039498540225\n",
      "Epoch: 126, Loss: 0.6650503280456243, Val Loss: 0.661977603286098\n",
      "Epoch: 127, Loss: 0.6637240219959246, Val Loss: 0.6606666738828576\n",
      "Epoch: 128, Loss: 0.662412477341922, Val Loss: 0.6593706687933812\n",
      "Epoch: 129, Loss: 0.6611153443903663, Val Loss: 0.6580892287263671\n",
      "Epoch: 130, Loss: 0.6598324448107927, Val Loss: 0.6568221095053883\n",
      "Epoch: 131, Loss: 0.6585634224259714, Val Loss: 0.6555688402332569\n",
      "Epoch: 132, Loss: 0.6573082713077112, Val Loss: 0.654328703078535\n",
      "Epoch: 133, Loss: 0.6560669091842138, Val Loss: 0.6531015661655168\n",
      "Epoch: 134, Loss: 0.6548386617801302, Val Loss: 0.6518876340631946\n",
      "Epoch: 135, Loss: 0.6536231251847119, Val Loss: 0.6506864896026305\n",
      "Epoch: 136, Loss: 0.6524202511758291, Val Loss: 0.6494979538534635\n",
      "Epoch: 137, Loss: 0.6512300114122123, Val Loss: 0.6483220546081203\n",
      "Epoch: 138, Loss: 0.6500519378989521, Val Loss: 0.6471586358910217\n",
      "Epoch: 139, Loss: 0.6488860081612566, Val Loss: 0.646007265207393\n",
      "Epoch: 140, Loss: 0.6477320387656974, Val Loss: 0.6448680310527221\n",
      "Epoch: 141, Loss: 0.6465898547394089, Val Loss: 0.6437405792460572\n",
      "Epoch: 142, Loss: 0.6454592220549138, Val Loss: 0.6426250199423882\n",
      "Epoch: 143, Loss: 0.6443400098844094, Val Loss: 0.6415206681029317\n",
      "Epoch: 144, Loss: 0.6432322417021025, Val Loss: 0.6404270235586179\n",
      "Epoch: 145, Loss: 0.6421355919042341, Val Loss: 0.6393440980856753\n",
      "Epoch: 146, Loss: 0.6410496771834525, Val Loss: 0.6382714680653375\n",
      "Epoch: 147, Loss: 0.6399742094692954, Val Loss: 0.6372085832884309\n",
      "Epoch: 148, Loss: 0.6389086820014905, Val Loss: 0.6361556602301468\n",
      "Epoch: 149, Loss: 0.6378531389941866, Val Loss: 0.6351122012874956\n",
      "Epoch: 150, Loss: 0.6368074191211627, Val Loss: 0.6340788310515043\n",
      "Epoch: 151, Loss: 0.6357717987679333, Val Loss: 0.6330550009743983\n",
      "Epoch: 152, Loss: 0.6347456640822444, Val Loss: 0.6320409006000828\n",
      "Epoch: 153, Loss: 0.6337291490967375, Val Loss: 0.6310362239703574\n",
      "Epoch: 154, Loss: 0.6327220203987537, Val Loss: 0.630041221741298\n",
      "Epoch: 155, Loss: 0.6317241296023234, Val Loss: 0.6290551279873464\n",
      "Epoch: 156, Loss: 0.6307352260172967, Val Loss: 0.6280781741936342\n",
      "Epoch: 157, Loss: 0.6297553533018211, Val Loss: 0.6271100292455626\n",
      "Epoch: 158, Loss: 0.6287843512593936, Val Loss: 0.6261509168780691\n",
      "Epoch: 159, Loss: 0.6278220109811032, Val Loss: 0.6252008296154166\n",
      "Epoch: 160, Loss: 0.6268684357667376, Val Loss: 0.6242587140922436\n",
      "Epoch: 161, Loss: 0.6259228380630667, Val Loss: 0.6233247033891669\n",
      "Epoch: 162, Loss: 0.6249853646783381, Val Loss: 0.6223989609958488\n",
      "Epoch: 163, Loss: 0.6240557774216767, Val Loss: 0.6214812537298302\n",
      "Epoch: 164, Loss: 0.6231341478504079, Val Loss: 0.6205714520483361\n",
      "Epoch: 165, Loss: 0.6222204215271746, Val Loss: 0.6196699263727825\n",
      "Epoch: 166, Loss: 0.6213145514089053, Val Loss: 0.61877643029097\n",
      "Epoch: 167, Loss: 0.6204163183943365, Val Loss: 0.6178901794621956\n",
      "Epoch: 168, Loss: 0.6195253344881654, Val Loss: 0.6170111376901494\n",
      "Epoch: 169, Loss: 0.6186413083232737, Val Loss: 0.6161393935789249\n",
      "Epoch: 170, Loss: 0.6177644664294855, Val Loss: 0.6152749002858311\n",
      "Epoch: 171, Loss: 0.616894945964515, Val Loss: 0.6144176454308825\n",
      "Epoch: 172, Loss: 0.6160325934483075, Val Loss: 0.6135674514454268\n",
      "Epoch: 173, Loss: 0.6151771792990393, Val Loss: 0.6127245320170673\n",
      "Epoch: 174, Loss: 0.6143286671041512, Val Loss: 0.6118884590909914\n",
      "Epoch: 175, Loss: 0.613486964191903, Val Loss: 0.6110588641454635\n",
      "Epoch: 176, Loss: 0.6126518741349996, Val Loss: 0.6102363262561562\n",
      "Epoch: 177, Loss: 0.6118232078396509, Val Loss: 0.6094205193705127\n",
      "Epoch: 178, Loss: 0.6110008129699644, Val Loss: 0.6086109620148542\n",
      "Epoch: 179, Loss: 0.6101846444569393, Val Loss: 0.6078076528134339\n",
      "Epoch: 180, Loss: 0.6093748465864013, Val Loss: 0.6070107845058763\n",
      "Epoch: 181, Loss: 0.608571528679491, Val Loss: 0.6062204482692801\n",
      "Epoch: 182, Loss: 0.6077746122822525, Val Loss: 0.6054365551244626\n",
      "Epoch: 183, Loss: 0.606983804941424, Val Loss: 0.6046589333909648\n",
      "Epoch: 184, Loss: 0.6061991522232092, Val Loss: 0.6038876792695773\n",
      "Epoch: 185, Loss: 0.6054205360685211, Val Loss: 0.6031226574422595\n",
      "Epoch: 186, Loss: 0.6046479804217795, Val Loss: 0.6023636416311126\n",
      "Epoch: 187, Loss: 0.6038811828931622, Val Loss: 0.6016102407853021\n",
      "Epoch: 188, Loss: 0.6031201575781673, Val Loss: 0.6008625563303961\n",
      "Epoch: 189, Loss: 0.6023650713853349, Val Loss: 0.6001206314118712\n",
      "Epoch: 190, Loss: 0.6016159463033252, Val Loss: 0.5993847406150181\n",
      "Epoch: 191, Loss: 0.6008726578422146, Val Loss: 0.5986537725923811\n",
      "Epoch: 192, Loss: 0.6001346010396659, Val Loss: 0.5979281123262957\n",
      "Epoch: 193, Loss: 0.5994016559499981, Val Loss: 0.5972076837304136\n",
      "Epoch: 194, Loss: 0.5986738302712465, Val Loss: 0.5964922436880903\n",
      "Epoch: 195, Loss: 0.5979510355067571, Val Loss: 0.5957819273201088\n",
      "Epoch: 196, Loss: 0.5972332774123311, Val Loss: 0.5950767665951764\n",
      "Epoch: 197, Loss: 0.5965204429688719, Val Loss: 0.5943764378503549\n",
      "Epoch: 198, Loss: 0.5958125284232173, Val Loss: 0.593680923951061\n",
      "Epoch: 199, Loss: 0.5951096391535353, Val Loss: 0.5929905704505671\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7965\n"
     ]
    }
   ],
   "source": [
    "# This is where the class is used to train the model\n",
    "\n",
    "# The parameters in the model are (input_neurons, hidden_neurons, output_neurons, learning_rate, epochs)\n",
    "# These parameters aren't the right parameters, so tweak them to get the best results\n",
    "# Around 70% accuracy is a good end goal (75% is great) but for the recruitment task, 60% is good enough\n",
    "\n",
    "# [Q17] What are the parameters in the model and what do they mean?\n",
    "\n",
    "fashion_mnist = NN(784, 256, 10, 0.05, 200)\n",
    "p = np.random.permutation(len(X))\n",
    "X, y = X[p], y[p]\n",
    "\n",
    "# Splitting the data into training, validation and testing in the ratio 70:20:10\n",
    "X_train, y_train = X[:int(0.7*len(X))], y[:int(0.7*len(X))]\n",
    "X_val, y_val = X[int(0.7*len(X)):int(0.9*len(X))], y[int(0.7*len(X)):int(0.9*len(X))]\n",
    "X_test, y_test = X[int(0.9*len(X)):], y[int(0.9*len(X)):]\n",
    "\n",
    "# Training the model\n",
    "train_loss,val_loss = fashion_mnist.fit(X_train, y_train,X_val,y_val)\n",
    "\n",
    "\n",
    "# Plotting the loss\n",
    "plt.plot(train_loss,label='train')\n",
    "plt.plot(val_loss,label='val')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "y_pred = fashion_mnist.predict(X_test)\n",
    "\n",
    "# [Q18] Why are we using argmax here? Why is this output different from the output of the model?\n",
    "y_pred = np.argmax(y_pred, axis=1)  \n",
    "y_test = np.argmax(y_test, axis=1)\n",
    "print(f\"Accuracy: {np.mean(y_pred == y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aedae7b-55da-4b69-ae70-f268ec6794ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
